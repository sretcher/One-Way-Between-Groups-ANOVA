---
title: '**One-Way Between-Groups ANOVA**'
output: rmarkdown::github_document
---

A clinical study was conducted to assess the effect of three formulations of the same drug on reducing cholesterol. The formulations were 20mg at once (1time), 10mg twice a day (2times), and 5mg four times a day (4times). In addition, two competing drugs were used as a control group (drugD and drugE). Fifty patients were randomly selected for the study and 10 patients were randomly assigned to each treatment. The purpose of the study was to find which of the formulations, if any, were the most effective at reducing cholesterol and how these formulations compare with the existing drugs. We will do this by conducting a One-Way ANOVA F-Test to compare treatment means in a Completely Randomized Design. 


### **Examining the Data**
The following code loads the packages needed for the analysis.


```{r results='hide', message=FALSE, warning=FALSE}

library(multcomp)
library(tidyverse)
library(rstatix)
library(ggpubr)
library(gplots)
library(car)
```

The table below shows how some lesser known packages are used in the analysis. 

Package| Usage in Project
------------- | -------------
`multcomp` | Our data comes from the dataset *cholesterol* in `multcomp`. This package also provides methods for multiple mean comparisons between treatments which will be used later in the analysis.
`rstatix` | `rstatix` provides a framework that allows us to use statistics with the tidyverse by providing pipeable statistical functions. 
`ggpubr`/`gplots` | `ggpubr` and `gplots` allow us to visualize data easier instead of spending forever customizing `ggplot2` plots.
`car`| `car` has functions which are useful for detecting outliers in our data.

Our data comes from the dataset *cholesterol* in `multcomp`. The following code coverts the dataset to a `tibble`, which makes it easier to use the `tidyverse` with our data.
```{r}

cholesterol <- tibble(cholesterol)


```


The `head` function is used to get an idea of how the data is stored. The levels of the factor `trt` represent the five different drug formulations which are beleived to have an effect on our response variable (cholesterol reduction).
```{r}

head(cholesterol)


```


To examine the treatments of the experiment, we use the function `levels` on our factor `trt`. The treatments appear to be in order and spelled correctly as stated in the introduction. 
```{r}

levels(cholesterol$trt)


```


The function `sample_n_by` is used to return two random rows from each treatment. This gives us a better overview of data and all of the treatments found by the `levels` function are included.
```{r}


cholesterol %>% 
sample_n_by(trt, size = 2)

```


We use the `table` function to find the sample size for each treatment. In general, when sample sizes are unequal across treatments, ANOVA becomes more complicated.

Having unequal sample sizes can affect the homogeneity of variance assumption which will result in a lower statistical power for the test. Looking below, we see that our experiment has a balanced design with 10 experimental units for each treatment.  
```{r}

table(cholesterol$trt)


```


The function `summary` is used to view the distribution of our response variable. 
```{r}

summary(cholesterol$response)


```


The following code computes the mean, standard deviation, and sample size for each treatment. 
```{r}


group_stats <- cholesterol %>%
              group_by(trt) %>%
              summarise(n=n(),group_mean = mean(response),group_sd = sd(response))

group_stats
```



```{r}



ggplot(mapping=aes(trt,response),cholesterol)+
  geom_boxplot()

```

### **Conditions Required for a Valid Anova F-Test: Completely Randomized Design**

1. The samples are randomly selected in an independent manner from the treatment populations.
2. All k sampled populations have distribution that are approximately normal.
3. The k population variances are equal.

A patient is the experimental unit for this experiment. For a Completely Randomized Design, independent random samples of experimental units are selected for each treatment or k treatments can be randomly assigned to a random sample of experimental units. Our experiment has the latter, since 50 patients were randomly selected and than randomly assigned to treatments. We will consider the first assumption to be fulfilled.

For the second assumption, we can either check the normality of the residuals of our ANOVA model or we can check the normality of each treatment population. The following code uses the function `ggqqplot` function to plot a Q-Q plot of the response variable for each treatment. Looking at the plots, we can see that the k populations are normally distributed. 





```{r}



ggqqplot(cholesterol, "response", facet.by = "trt")


```

Recall that Anova and Regression are both special cases of the general linear model, which is why we can use the same `lm` function for regression to find the residuals for ANOVA. We can then conduct the Shapiroâ€“Wilk test on the model's residuals to test for normality.  Since the p-value is larger than our significance level(0.05), we do not reject the null hypothesis which means our k treatment populations are most likely normal.

```{r}

model <- lm(response ~ trt,data = cholesterol)
shapiro.test(model$residuals)

```


The following code make a Q-Q plot of the above model's residuals.
```{r}


qqnorm(model$residuals)
qqline(model$residuals)


```



The following code computes the Shapiro-Wilk test for each treatment. Since we have large p-values, we have enough evidence to prove that the treatment populations are normal. 
```{r}


cholesterol %>%
  group_by(trt) %>%
  summarize(W=shapiro.test(response)$statistic, p_value=shapiro.test(response)$p.value)

```


The following code does the same thing as the code above but it is a lot cleaner. The `shapiro_test` function is from the package `rstatix` which allows us to use the pipe with statistical tests. 
```{r}


cholesterol %>%
  group_by(trt) %>%
  shapiro_test(response)


```


Looking at all the code above, we will consider the second assumption to be fulfilled. For the third assumptions we can use Bartlett's Test of Homogeneity of Variances to find out if the k population variances are equal. We are using Bartlett's test here instead of Levene's test because Bartlett's test performs better when we know that our data is normal. Since the p-value is large for Bartlett's test, the variances among treatments do not differ significantly.  

```{r}


bartlett.test(response~trt,data = cholesterol)


```




Outliers inflate the MST and MSE, which leads to a smaller F Statistic. This means that the chance of rejecting the null hypothesis is also lowered. It's important to make sure there are no data collection errors and to see the effect of outliers on our analysis. Using the `identify_outliers` function allows us to identify outliers using boxplot methods. Looking below, we do not have any extreme outliers so we will keep the data the same.
```{r}

cholesterol %>% 
  group_by(trt) %>%
  identify_outliers(response)


```


### **Hypotheses for ANOVA F-Test to Compare k Treatment Means**

We will denote the population means of k treatments as u1, u2, u3,... and so on. We will test the null hypothesis that treatment means are equal against the alternative hypothesis that at least two treatment means differ. We will use a level of significance of .05 for this test.

Ho: u1 = u2 = u3 = u4 = u5

Ha: At least two of these treatment means differ.

### **Conducting the Test**

To conduct a one-way ANOVA test, we use the `aov` function which requires a formula and a dataset. The formula below tells R to conduct the ANOVA test where response is the response variable and trt is the factor. We use the `summary` function on `fit` to summarize the results from the ANOVA tets. 

Looking at the results, we see that our F test statistic is quite big and our p-value is small. Recall that the F statistic is the average variation between treatment means over the average variation within treatments. The bigger the F statistic is, the more likely there is statistically significant difference between means. Since the p-value is smaller than our alpha and the F statistic is quite large, we can reject the null hypotheses and we have enough evidence that to prove that the treatment means are not equal.   
```{r}



fit <- aov(response ~ trt,data=cholesterol)
summary(fit)

```


Since we rejected the null hypothesis, we know that at least two treatment means differ but we are not sure which treatments differ significantly between each other. To make multiple comparisons of treatment means, we will use Tukey's method which works best with equal sample sizes and pairwise comparisons. 

Before we use Tukey's Method, recall that two independent sample t-tests are equivalent to two independent sample f-tests. Because of this, we can construct a one sample 95% confidence interval for each treatment. If two treatment's confidence intervals intersect, they are not significantly different. 

The `plotmeans` function plots the mean and ci for each treatment. From this plot, we see that 4times is the best drug formulation and is significantly different from 1time. We also see that drugE is statistically better different than the other treatments. We also see that 2times is not signifcantly different from 1times or 4times. Finally we see that DrugD is statistically different from 2times , 1time, and drugE but not statistically different from 4times.  
```{r}


plotmeans(cholesterol$response~cholesterol$trt, ci.label = TRUE,n.label = FALSE)

```


The following code shows the confidence intervals above are two-sided t-test confidence intervals. The two-sided t-test confidence interval for the treatment 1time is the same as the one above.
```{r}



g <- filter(cholesterol,trt=="1time")
t.test(x =g$response, conf.level = .95, alternative = "two.sided")

```

Why don't we just conduct multiple t-tests to find the confidence intervals between groups? The problem with this and the approach above is that it increases the likelihood of a Type 1 error. Using Tukey's method, we can say that collectively 95% of the intervals include the true difference between groups. This is because Tukey's method adjusts for the amount of groups being compared unlike the t-test. 

Looking below, any p-value below our significance level means those two groups are significant different. A 95% confidence interval is also provided which contains the true difference between those groups. For example, we that drugE-drugD has a low p-value meaning they are significantly different. Looking at the ci, we see that the true mean of drugE is (1.48,9.68) higher than drugd.  

```{r}

TukeyHSD(fit)


```

The following code shows another way to compute tukey's method for comparing treatments.

```{r}

pwc <- cholesterol %>% tukey_hsd(response~trt)
pwc

```


The following code visualizes the confidence intervals of differences between group means. If a confidence interval includes 0, those groups are not statistically different from each other. 
```{r}

par(las=2,mar = c(7,7,7,7))
plot(TukeyHSD(fit))


```


Finally, we use the following code which gives us the boxplots for each treatment along with Tukey's compact letter ranking. If two boxplots have the same letter, they are not statistically different. Overall, 4times is the best formulation for a certain drug for reducing cholesterol but falls short when compared to the drug E. 
```{r, out.height=20}

par(las=1,mar=c(6,6,6,6))
tuk <- glht(fit,linfct=mcp(trt="Tukey"))
plot(cld(tuk,level = .05),col = "lightblue")

```

